{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to authorise our app to access Twitter on our behalf, we need to use the OAuth interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = 'YOUR-CONSUMER-KEY'\n",
    "consumer_secret = 'YOUR-CONSUMER-SECRET'\n",
    "access_token = 'YOUR-ACCESS-TOKEN'\n",
    "access_secret = 'YOUR-ACCESS-SECRET'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<name_variable> = api.home_timeline()\n",
    "for status in <name_variable>:\n",
    "    print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can read our own timeline (i.e. our Twitter homepage) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in tweepy.xxxx(api.home_xxxx).items(10):\n",
    "    # Process a single status\n",
    "    print(status.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy provides the convenient Cursor interface to iterate through different types of objects. In the example above we’re using 10 to limit the number of tweets we’re reading, but we can of course access more. The status variable is an instance of the Status() class, a nice wrapper to access the data. The JSON response from the Twitter API is available in the attribute _json (with a leading underscore), which is not the raw JSON string, but a dictionary.\n",
    "\n",
    "So the code above can be re-written to process/store the JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    print(status.xxxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to have a list of all our followers? There you go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for friend in tweepy.Cursor(api.xxxx).items():\n",
    "    print(friend._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how about a list of all our tweets? Simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweepy.Cursor(api.user_xxxx).items():\n",
    "    print(tweet._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect 2000 @TelUniversity tweets and then saving the texts, number of retweets, and number of likes inside a list, we can perform as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telu_cursor = tweepy.Cursor(api.user_timeline, screen_name = 'TelU twitter account')\n",
    "telu_tweets = [(tweet.text, tweet.retweet_count, tweet.favorite_count) \\\n",
    "                 for tweet in telu_cursor.items(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telu_tweets[0]  # index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing tweet form account @<your account>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = api.user_timeline('<your twitter account>')\n",
    "for status in variable:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have seen how we can use Tweepy to collect actual Twitter data. Now let’s be curious and visualize them using Matplotlib.\n",
    "\n",
    "### Minimum Data Visualization\n",
    "\n",
    "To start this, let us see an example of plotting the username frequency of tweets in our home timeline. The username frequency is the number of tweets from a specific username that appears in our data. The tweets data will be of 200 tweets from the home timeline. Below is the code sample followed by the bar plot result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xxxxx as xxxx\n",
    "home_cursor = tweepy.Cursor(api.home_timeline)\n",
    "tweets = [i.author.screen_name for i in home_cursor.items(200)]\n",
    "unq = set(tweets)\n",
    "freq = {uname: tweets.count(uname) for uname in unq}\n",
    "plt.bar(range(len(unq)), freq.values())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the plot shows that there were xxx different Twitter accounts tweeting in the timeline, and there is only one account that tweets the most (xxx tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1. Objective: accounts comparison, Model: modified horizontal bar chart\n",
    "\n",
    "In this case, we will use a data of 192 tweets that I have collected from my home timeline a few months ago. The data will be transformed such that we can group the tweets by accounts, and then we will compare the numbers through a data visualization. It is important to note that we will develop a good data visualization that combines various Matplotlib features and not restrict ourselves to the plotting models it has.\n",
    "The modules that we will use in Python are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import operator\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for data representation is shown below. It will group the tweets by username and then count the tweets count (also can be seen as username frequency) and the number of followers for each username. It will also sort the data by the username frequency.\n",
    "\n",
    "### Download testfile.npy on https://github.com/rizaldp/BigDataDTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_tweets = list(numpy.load('testfile.npy'))\n",
    "unicity_key = operator.attrgetter('author.screen_name')\n",
    "tweets = sorted(home_tweets, key=unicity_key)\n",
    "authors_tweets = {}\n",
    "followers_count = {}\n",
    "sample_count = {}\n",
    "        \n",
    "for screen_name, author_tweets in itertools.groupby(tweets, key=unicity_key):\n",
    "    author_tweets = list(author_tweets)\n",
    "    author = author_tweets[0].author\n",
    "    authors_tweets[screen_name] = author\n",
    "    followers_count[screen_name] = author.followers_count\n",
    "    sample_count[screen_name] = len(author_tweets)\n",
    "maxfolls = max(followers_count.values())\n",
    "sorted_screen_name = sorted(sample_count.keys(), key = lambda x: sample_count[x])\n",
    "sorted_folls = [followers_count[i] for i in sorted_screen_name]\n",
    "sorted_count = [sample_count[i] for i in sorted_screen_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we could easily perform the horizontal bar plot byplt.hbar(range(len(sorted_count)),sorted_count)and then setting the titles for the figure and x-y axis, but let us improvise. We will add additional visual form by adding color transparancy difference, for example: the transparancy of the bars may represent the number of followers of the accounts. We will also change the tick labels on the y axis from numbers to strings of usernames. Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(0.1, 0.1, 1, i/maxfolls) for i in sorted_folls]\n",
    "fig, ax = plt.subplots(1,1)\n",
    "y_pos = range(len(colors))\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(sorted_screen_name, font_properties = helv_8)\n",
    "ax.barh(y_pos, sorted_count, \\\n",
    "        height = 0.7, left = 0.5, \\\n",
    "        color = colors)\n",
    "for i in y_pos:\n",
    "    ax.text(sorted_count[i]+1, i, style_the_num(str(sorted_folls[i])), \\\n",
    "            font_properties = helv_10, color = colors[i])\n",
    "ax.set_ylim(-1, y_pos[-1]+1+0.7)\n",
    "ax.set_xlim(0, max(sorted_count) + 8)\n",
    "ax.set_xlabel('Tweets count', font_properties = helv_14)\n",
    "plt.tight_layout(pad=3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistweepy import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "stats = numpy.load('ProSyn.npy') #You can get this data from the 'samples' folder.\n",
    "tweets = models.Tweets(stats)\n",
    "model = models.Authors(tweets)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "model.hbar_plot(ax, meas = 'total_tweets', \\\n",
    "                incolor_meas = 'total_tweets', \\\n",
    "                text_sizes = [10, 15], \\\n",
    "                freq_lim = (10000, 1000000)) \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "In case we want to “keep the connection open”, and gather all the upcoming tweets about a particular event, the streaming API is what we need. We need to extend the StreamListener() to customise the way we process the incoming data. A working example that gathers all the new tweets with the #python hashtag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('xxxxx.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['#hashtag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that you have collected a number of tweets and stored them in JSON as suggested in the previous article, let’s have a look at the structure of a tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "with open('xxxxx.json', 'r') as f:\n",
    "    line = f.readline() # read only the first tweet/line\n",
    "    tweet = json.loads(line) # load it as Python dict\n",
    "    print(json.dumps(tweet, indent=4)) # pretty-print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Tokenise a Tweet Text\n",
    "Let’s see an example, using the popular NLTK library to tokenise a fictitious tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, @-mentions, emoticons, URLs and #hash-tags are now preserved as individual tokens.\n",
    "\n",
    "If we want to process all our tweets, previously saved on file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "fname = 'xxxxx.json'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_all)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the nature of our data and our tokenisation, we should also be careful with all the punctuation marks and with terms like RT (used for re-tweets) and via (used to mention the original author of an article or a re-tweet), which are not in the default stop-word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now substitute the variable terms_all in the first example with something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "print(terms_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides stop-word removal, we can further customise the list of terms/tokens we are interested in. Here you have some examples that you can embed in the first fragment of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count terms only once, equivalent to Document Frequency\n",
    "terms_single = set(terms_all)\n",
    "print(\"Terms single: \", terms_single)\n",
    "\n",
    "# Count hashtags only\n",
    "terms_hash = [term for term in preprocess(tweet['text']) \n",
    "              if term.startswith('#')]\n",
    "print(\"\\nTerms hashtag: \",terms_hash)\n",
    "\n",
    "# Count terms only (no hashtags, no mentions)\n",
    "terms_only = [term for term in preprocess(tweet['text']) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))] \n",
    "              # mind the ((double brackets))\n",
    "              # startswith() takes a tuple (not a list) if \n",
    "              # we pass a list of inputs\n",
    "print(\"\\nTerms Only: \",terms_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the other frequent terms represent a clear topic, more often than not simple term frequencies don’t give us a deep explanation of what the text is about. To put things in context, let’s consider sequences of two terms (a.k.a. bigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams \n",
    " \n",
    "terms_bigram = bigrams(terms_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigrams() function from NLTK will take a list of tokens and produce a list of tuples using adjacent tokens. Notice that we could use terms_all to compute the bigrams, but we would probably end up with a lot of garbage. In case we decide to analyse longer n-grams (sequences of n tokens), it could make sense to keep the stop-words, just in case we want to capture phrases like “to be or not to be”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# remember to include the other import from the previous post\n",
    " \n",
    "com = defaultdict(lambda : defaultdict(int))\n",
    " \n",
    "# f is the file pointer to the JSON data set\n",
    "for line in f: \n",
    "    tweet = json.loads(line)\n",
    "    terms_only = [term for term in preprocess(tweet['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))]\n",
    " \n",
    "    # Build co-occurrence matrix\n",
    "    for i in range(len(terms_only)-1):            \n",
    "        for j in range(i+1, len(terms_only)):\n",
    "            w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "            if w1 != w2:\n",
    "                com[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
